{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title: 2-layered Neural Network\n",
    "\n",
    "### Date: November 29, 2019\n",
    "### Author: Nikhil Singh\n",
    "### Details: The entire code of neural networks has been implemented using numpy matrix and has been tested 3-input X-OR gate.\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics of Neural Network\n",
    "\n",
    "The output $\\widehat y$ of a simple 2-layer Neural Network is:\n",
    "\n",
    "$$\\widehat y = \\sigma(W_{2}\\sigma(W_{1}x + b_{1}) + b_{2})$$\n",
    "\n",
    "\n",
    "> Each iteration of the training process consists of the following steps:\n",
    "    - Calculating the predicted output Å·, known as feedforward\n",
    "    - Updating the weights and biases, known as backpropagation\n",
    "    \n",
    "-------    \n",
    "### Feedforward\n",
    "$$\\widehat y = \\sigma(W_{2}\\sigma(W_{1}x + b_{1}) + b_{2})$$\n",
    "\n",
    "-------    \n",
    "### Loss Function\n",
    "$$Sum of Squares Error = \\sum_{i=1}^{n} (y-\\widehat y)^2$$\n",
    "\n",
    "-------    \n",
    "### Backpropogation\n",
    "- derivative of the loss function w.r.t the weights and biases. The issue is, we can't directly calculate the derivative of the loss function w.r.t. to the weights and biases because the equation of the loss function doesn't contant the weights and biases.\n",
    "\n",
    "> Chain rule     \n",
    " $$ Loss(y,\\widehat y) = \\sum_{i=1}^{n}(y-\\widehat y)^2$$\n",
    " $$ \\frac{\\partial Loss(y,\\widehat y)}{\\partial W} = \\frac{\\partial Loss(y, \\widehat y)}{\\partial \\widehat y} * \\frac{\\partial \\widehat y}{\\partial z} * \\frac{\\partial z}{\\partial W}$$\n",
    " $$ =   2(y - \\widehat y) * derivative     of sigmoid function *x$$\n",
    " $$ =   2(y - \\widehat y) * z(1-z) *x$$\n",
    " \n",
    " _above shown is a partial derivative on 1-layer NN_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "\n",
    "## Below is the python code of the same and I've coded everything from scratch:\n",
    "\n",
    "### __Objective:__ Implementing a XOR gate using Neural Network with 3 inputs and trying to get as close to 100% accuracy as possible using a 2 layer Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All the necessary functions are coded below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1.0 - x)\n",
    "\n",
    "def initialize_wt_bias(input_layer_neurons, hidden_layer_neurons, output_neurons):\n",
    "    # for hidden layer\n",
    "    weights_hidden = np.random.uniform(size = (input_layer_neurons, hidden_layer_neurons))\n",
    "    bias_hidden = np.random.uniform(size = (1, hidden_layer_neurons))\n",
    "    \n",
    "    # for output layer (single neuron in this case, can be treated for multiple cases also)\n",
    "    weights_output = np.random.uniform(size = (hidden_layer_neurons, output_neurons))\n",
    "    bias_output = np.random.uniform(size = (1, output_neurons))\n",
    "    return weights_hidden, bias_hidden, weights_output, bias_output\n",
    "    \n",
    "def feedforward(X, weights_hidden, bias_hidden, weights_output, bias_output):\n",
    "    # Forward Propogation\n",
    "    \n",
    "    hidden_layer_ip = np.dot(X, weights_hidden) + bias_hidden # dot-product of input & weights for hidden layer and adding bias in that\n",
    "    hidden_layer_activation = sigmoid(hidden_layer_ip)\n",
    "    \n",
    "    output_layer_ip = np.dot(hidden_layer_activation, weights_output) + bias_output\n",
    "    output_layer_activation = sigmoid(output_layer_ip)\n",
    "    return hidden_layer_activation, output_layer_activation\n",
    "    \n",
    "def loss_function(y, output_layer_activation):\n",
    "    error = y - output_layer_activation\n",
    "    return error\n",
    "\n",
    "def backpropogation(X, hidden_layer_activation, output_layer_activation, error, weights_output, bias_output, weights_hidden, bias_hidden, learning_rate = 0.1):\n",
    "    # slope from output layer\n",
    "    slope_output_layer = sigmoid_derivative(output_layer_activation)\n",
    "    d_output = error * slope_output_layer\n",
    "    \n",
    "    # slope from hidden layer\n",
    "    slope_hidden_layer = sigmoid_derivative(hidden_layer_activation)\n",
    "    error_hidden_layer = np.dot(d_output, weights_output.T)\n",
    "    d_hidden_layer = error_hidden_layer * slope_hidden_layer\n",
    "    \n",
    "    # Re-initializing output weights & bias matrix\n",
    "    weights_output += np.dot(hidden_layer_activation.T, d_output) * learning_rate\n",
    "    bias_output += np.sum(d_output, axis=0, keepdims=True) * learning_rate\n",
    "    \n",
    "    # Re-initialzing hidden layer weights & bias matrix\n",
    "    weights_hidden += np.dot(X.T, d_hidden_layer) * learning_rate\n",
    "    bias_hidden += np.sum(d_hidden_layer, axis=0, keepdims=True) * learning_rate\n",
    "    \n",
    "    return weights_hidden, bias_hidden, weights_output, bias_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The truth-table for a 3-input XOR (Exclusive - OR) gate looks like this\n",
    "\n",
    "| X1 | X2 | X3 | y |\n",
    "|:--:|:--:|:--:|:-:|\n",
    "|  0 |  0 |  0 | 0 |\n",
    "|  0 |  0 |  1 | 1 |\n",
    "|  0 |  1 |  0 | 1 |\n",
    "|  0 |  1 |  1 | 0 |\n",
    "|  1 |  0 |  0 | 1 |\n",
    "|  1 |  0 |  1 | 0 |\n",
    "|  1 |  1 |  0 | 0 |\n",
    "|  1 |  1 |  1 | 1 |\n",
    "\n",
    "> I'll use _X1, X2 & X3_ as inputs and _y_ as output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Input:\n",
      "[[0 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 1]\n",
      " [1 0 0]\n",
      " [1 0 1]\n",
      " [1 1 0]\n",
      " [1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "X=np.array([[0,0,0],[0,0,1],[0,1,0],[0,1,1],[1,0,0],[1,0,1],[1,1,0],[1,1,1]])\n",
    "print ('\\n Input:')\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actual Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Actual Output:\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "y = np.array([[0],[1],[1],[0],[1],[0],[0],[1]])\n",
    "print('\\n Actual Output:')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing basic arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 10000\n",
    "learning_rate = 0.1\n",
    "input_layer_neurons = X.shape[1]\n",
    "hidden_layer_neurons = 8\n",
    "output_neurons = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing Weights & biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_hidden, bias_hidden, weights_output, bias_output = initialize_wt_bias(input_layer_neurons, hidden_layer_neurons, output_neurons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the neural networks by simply calling the above mentioned functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Epoch 1000\n",
      "Epoch 2000\n",
      "Epoch 3000\n",
      "Epoch 4000\n",
      "Epoch 5000\n",
      "Epoch 6000\n",
      "Epoch 7000\n",
      "Epoch 8000\n",
      "Epoch 9000\n"
     ]
    }
   ],
   "source": [
    "for i in range(epoch):\n",
    "    \n",
    "    if i%1000 == 0:\n",
    "        print(\"Epoch {}\".format(i))\n",
    "    \n",
    "    hidden_layer_activation, output_layer_activation = feedforward(X, weights_hidden, bias_hidden, weights_output, bias_output)\n",
    "    error = loss_function(y, output_layer_activation)\n",
    "    weights_hidden, bias_hidden, weights_output, bias_output = backpropogation(X, hidden_layer_activation, output_layer_activation, error, weights_output, bias_output, weights_hidden, bias_hidden, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Output from the model:\n",
      "[[0.09779328]\n",
      " [0.88350865]\n",
      " [0.87770153]\n",
      " [0.26781581]\n",
      " [0.88290741]\n",
      " [0.26951997]\n",
      " [0.26971992]\n",
      " [0.48599104]]\n"
     ]
    }
   ],
   "source": [
    "print ('\\n Output from the model:')\n",
    "print (output_layer_activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model predicted output array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = np.where(output_layer_activation>0.5,1,0)\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 87.5\n"
     ]
    }
   ],
   "source": [
    "accuracy = (y == y_hat).mean() * 100\n",
    "print(\"Accuracy = {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another Experiment with epochs = 15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Epoch 1000\n",
      "Epoch 2000\n",
      "Epoch 3000\n",
      "Epoch 4000\n",
      "Epoch 5000\n",
      "Epoch 6000\n",
      "Epoch 7000\n",
      "Epoch 8000\n",
      "Epoch 9000\n",
      "Epoch 10000\n",
      "Epoch 11000\n",
      "Epoch 12000\n",
      "Epoch 13000\n",
      "Epoch 14000\n",
      "\n",
      " Output from the model:\n",
      "[[0.0178696 ]\n",
      " [0.97641074]\n",
      " [0.977834  ]\n",
      " [0.04015204]\n",
      " [0.96394376]\n",
      " [0.03314883]\n",
      " [0.03616036]\n",
      " [0.95321119]]\n",
      "Accuracy = 100.0\n"
     ]
    }
   ],
   "source": [
    "# Initializing basic arguments\n",
    "epoch = 15000\n",
    "learning_rate = 0.1\n",
    "input_layer_neurons = X.shape[1]\n",
    "hidden_layer_neurons = 8\n",
    "output_neurons = 1\n",
    "\n",
    "# Initializing weights and biases\n",
    "weights_hidden, bias_hidden, weights_output, bias_output = initialize_wt_bias(input_layer_neurons, hidden_layer_neurons, output_neurons)\n",
    "\n",
    "# Running the model\n",
    "for i in range(epoch):\n",
    "    \n",
    "    if i%1000 == 0:\n",
    "        print(\"Epoch {}\".format(i))\n",
    "    \n",
    "    hidden_layer_activation, output_layer_activation = feedforward(X, weights_hidden, bias_hidden, weights_output, bias_output)\n",
    "    error = loss_function(y, output_layer_activation)\n",
    "    weights_hidden, bias_hidden, weights_output, bias_output = backpropogation(X, hidden_layer_activation, output_layer_activation, error, weights_output, bias_output, weights_hidden, bias_hidden, learning_rate)\n",
    "    \n",
    "    \n",
    "# Output probabilities\n",
    "print ('\\n Output from the model:')\n",
    "print (output_layer_activation)\n",
    "\n",
    "# Creating model predicted y-matrix\n",
    "y_hat = np.where(output_layer_activation>0.5,1,0)\n",
    "\n",
    "# Calculating Accuracy\n",
    "accuracy = (y == y_hat).mean() * 100#.all(axis=(0,2)).mean()\n",
    "print(\"Accuracy = {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> After running the code for _10000_ epochs which will not take more than 2 seconds to run, I'm getting an accuracy of 87.5%\n",
    "\n",
    "> In case of _15000_ epochs, I was able to achieve 100% accuracy.\n",
    "\n",
    "## Script Over"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
